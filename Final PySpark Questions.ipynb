{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed32ada-a82d-4aaf-b8ad-670512719b91",
   "metadata": {},
   "source": [
    "# Question3. How many fatal log entries in the months of December or January resulted from an ”invalid or missing program image”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e0f574-c0f6-44bd-8107-919cd25fed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fatal log entries in December or January resulting from 'invalid or missing program image': 18616\n",
      "Total execution time: 21.19 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, month\n",
    "import time\n",
    "\n",
    "#Intialize the Spark\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "      .config(\"spark.driver.host\", \"localhost\") \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"FatalProgramImageError\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "#Log File Path \n",
    "log_file_path = \"BGL\"\n",
    "\n",
    "\n",
    "log_rdd = spark.sparkContext.textFile(log_file_path)\n",
    "\n",
    "#Function to extract relevant data\n",
    "def extract_fatal_program_image(line):\n",
    "    columns = line.split()\n",
    "    if len(columns) >= 10:\n",
    "        timestamp_str = columns[1]\n",
    "        date_str = columns[2]\n",
    "        date_time_str = columns[4]\n",
    "        level = columns[8]\n",
    "        message_content = ' '.join(columns[9:])\n",
    "        if level == \"FATAL\" and \"invalid or missing program image\" in message_content:\n",
    "            return (date_time_str, timestamp_str)\n",
    "    return None\n",
    "\n",
    "#None values are filtered\n",
    "filtered_log_rdd = log_rdd.map(extract_fatal_program_image).filter(lambda x: x is not None)\n",
    "\n",
    "#RDD to DataFrame coversion\n",
    "log_df = filtered_log_rdd.toDF([\"DateTime\", \"Timestamp\"])\n",
    "\n",
    "\n",
    "log_df = log_df.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"long\"))\n",
    "log_df = log_df.withColumn(\"DateTime\", unix_timestamp(col(\"DateTime\"), 'yyyy-MM-dd-HH.mm.ss.SSSSSS').cast(\"timestamp\"))\n",
    "\n",
    "#Filtering the log entries and including onlt that occurred in December or January\n",
    "filtered_df = log_df.filter((month(col(\"DateTime\")) == 12) | (month(col(\"DateTime\")) == 1))\n",
    "\n",
    "#Count the number of relevant log entries\n",
    "fatal_count = filtered_df.count()\n",
    "\n",
    "#Showcasing the result\n",
    "print(f\"Number of fatal log entries in December or January resulting from 'invalid or missing program image': {fatal_count}\")\n",
    "\n",
    "#Recording the end time\n",
    "end_time = time.time()\n",
    "\n",
    "#Calculation and printing of the total execution time\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "\n",
    "#Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee89d2-3e19-4427-acff-feda5ea08d66",
   "metadata": {},
   "source": [
    "# Question12. What are the top 6 most frequently occurring hours in the log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9fac57b-1fa7-450f-8c62-d54f06f36466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Hour|TotalCount|\n",
      "+----+----------+\n",
      "|  11|    385826|\n",
      "|  10|    373509|\n",
      "|  12|    351982|\n",
      "|  08|    315017|\n",
      "|  09|    303681|\n",
      "|  17|    299275|\n",
      "+----+----------+\n",
      "\n",
      "Execution Time: 24.364806175231934 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"TopHours\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Log File Path\n",
    "log_file_path = \"BGL\"\n",
    "\n",
    "#log file into an RDD\n",
    "log_rdd = spark.sparkContext.textFile(log_file_path)\n",
    "\n",
    "#Defining the function to extract the hour\n",
    "def extract_hour(line):\n",
    "    columns = line.split()\n",
    "    if len(columns) >= 10:\n",
    "        date_time_str = columns[4]\n",
    "        hour = date_time_str.split('-')[3].split('.')[0]\n",
    "        return (hour, 1)\n",
    "    return None\n",
    "\n",
    "#Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "#Extracting relevant information and filtering out None values\n",
    "filtered_log_rdd = log_rdd.map(extract_hour).filter(lambda x: x is not None)\n",
    "\n",
    "#Converting the RDD to a DataFrame\n",
    "log_df = filtered_log_rdd.toDF([\"Hour\", \"Count\"])\n",
    "\n",
    "#Log hour count\n",
    "hour_counts_df = log_df.groupBy(\"Hour\").sum(\"Count\").withColumnRenamed(\"sum(Count)\", \"TotalCount\")\n",
    "\n",
    "#Obtaining the top 6 most frequently occurring hours\n",
    "top_6_hours_df = hour_counts_df.orderBy(col(\"TotalCount\").desc()).limit(6)\n",
    "\n",
    "#Displaying the result\n",
    "top_6_hours_df.show()\n",
    "\n",
    "#End time\n",
    "end_time = time.time()\n",
    "\n",
    "#Calculating and printing the execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time} seconds\")\n",
    "\n",
    "#Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114f7df-6647-4e51-bb54-5f1050ad195e",
   "metadata": {},
   "source": [
    "# Qustion14. Which node generated the largest number of KERNRTSP events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf9ca39-18dd-4197-b8a0-f1850c2a02e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node with the largest number of KERNRTSP events: R63-M0-NE-C:J12-U01 with 22 events\n",
      "Execution Time: 21.74234700202942 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "#Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"LogParser\").getOrCreate()\n",
    "\n",
    "#Log File Path\n",
    "log_file_path = \"BGL\"\n",
    "\n",
    "#log file into an RDD\n",
    "logs_rdd = spark.sparkContext.textFile(log_file_path)\n",
    "\n",
    "#Deining the function to parse each line\n",
    "def parse_line(line):\n",
    "    parts = line.split()\n",
    "    alert_message_flag = parts[0]\n",
    "    timestamp = parts[1]\n",
    "    date = parts[2]\n",
    "    node = parts[3]\n",
    "    date_time = parts[4]\n",
    "    node_repeated = parts[5]\n",
    "    ras_message_type = parts[6]\n",
    "    system_component = parts[7]\n",
    "    level = parts[8]\n",
    "    message_content = \" \".join(parts[9:])\n",
    "    return (alert_message_flag, timestamp, date, node, date_time, node_repeated, ras_message_type, system_component, level, message_content)\n",
    "\n",
    "#Recording the start time\n",
    "start_time = time.time()\n",
    "\n",
    "#Parsing the RDD\n",
    "parsed_logs_rdd = logs_rdd.map(parse_line)\n",
    "\n",
    "#Schema for the DataFrame\n",
    "columns = [\"alert_message_flag\", \"timestamp\", \"date\", \"node\", \"date_time\", \"node_repeated\", \"ras_message_type\", \"system_component\", \"level\", \"message_content\"]\n",
    "\n",
    "#Convert the RDD to a DataFrame\n",
    "logs_df = parsed_logs_rdd.toDF(columns)\n",
    "\n",
    "#Filtering the KERNRTSP events\n",
    "kernrtsp_logs_df = logs_df.filter(col(\"alert_message_flag\") == \"KERNRTSP\")\n",
    "\n",
    "#Count the number of KERNRTSP events per node\n",
    "node_counts_df = kernrtsp_logs_df.groupBy(\"node\").count()\n",
    "\n",
    "# Finding the node with the largest number of KERNRTSP events\n",
    "max_node_count = node_counts_df.orderBy(col(\"count\").desc()).first()\n",
    "\n",
    "#End time\n",
    "end_time = time.time()\n",
    "\n",
    "#Calculating the execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Node with the largest number of KERNRTSP events: {max_node_count['node']} with {max_node_count['count']} events\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")\n",
    "\n",
    "#Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ea16a",
   "metadata": {},
   "source": [
    "# Below Questions are using MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4c6e9",
   "metadata": {},
   "source": [
    "# Question 6. For each day of the week, what is the average number of seconds during which ”torus receiver z+ input pipe errors” were detected and corrected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8940a8-9df9-46de-aa4e-be716156c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "# Defining the function of Mapper and Reducer\n",
    "class MRTorusReceiverErrors(MRJob):\n",
    "    log_pattern = re.compile(r'- \\d+ \\d{4}\\.\\d{2}\\.\\d{2} .+ (\\d{4}-\\d{2}-\\d{2}-\\d{2}\\.\\d{2}\\.\\d{2})\\.\\d+ .+ RAS .+ torus receiver z\\+ input pipe error')\n",
    "\n",
    "    def mapper_extract_errors(self, _, line):\n",
    "        match = self.log_pattern.match(line)\n",
    "        if match:\n",
    "            timestamp_str = match.group(1)\n",
    "            timestamp = datetime.datetime.strptime(timestamp_str, '%Y-%m-%d-%H.%M.%S')\n",
    "            day_of_week = timestamp.strftime('%A')\n",
    "            yield day_of_week, float(timestamp.strftime('%S'))\n",
    "\n",
    "    def reducer_average_errors(self, day, counts):\n",
    "        total_counts = list(counts)\n",
    "        yield day, sum(total_counts) / len(total_counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_extract_errors,\n",
    "                   reducer=self.reducer_average_errors),\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRTorusReceiverErrors.run()\n",
    "    \n",
    "    # End the timer and print the total execution time\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total execution time: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915c3bb",
   "metadata": {},
   "source": [
    "# 19. On which date and time was the latest fatal app error where the message contains ”message prefix on control stream”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "# Defining the function of Mapper and Reducer phase\n",
    "class MRLatestFatalAppError(MRJob):\n",
    "\n",
    "    def mapper_extract_fatal_app_error(self, _, line):\n",
    "        columns = line.split()\n",
    "        if len(columns) >= 10:\n",
    "            timestamp_str = columns[1]\n",
    "            date_str = columns[2]\n",
    "            date_time_str = columns[4]\n",
    "            level = columns[8]\n",
    "            message_content = ' '.join(columns[9:])\n",
    "            if level == \"FATAL\" and \"message prefix on control stream\" in message_content:\n",
    "                yield None, (date_time_str, timestamp_str)\n",
    "\n",
    "    def reducer_find_latest(self, _, date_time_pairs):\n",
    "        latest_date_time = max(date_time_pairs, key=lambda x: int(x[1]))\n",
    "        yield \"Latest Fatal App Error\", latest_date_time\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_extract_fatal_app_error,\n",
    "                   reducer=self.reducer_find_latest)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRLatestFatalAppError.run()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution Time: {execution_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
